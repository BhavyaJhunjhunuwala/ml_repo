{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POffKTuzpFFW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('A2 - A2.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.dropna()"
      ],
      "metadata": {
        "id": "vTa38Ekwt6hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Define a function to classify sentiment\n",
        "def classify_sentiment(text, threshold=0.4):\n",
        "    analysis = TextBlob(text)\n",
        "    sentiment_score = analysis.sentiment.polarity\n",
        "    if sentiment_score >= threshold:\n",
        "        return 1  # Good experience\n",
        "    else:\n",
        "        return 0  # Bad experience\n",
        "\n",
        "# Assuming 'reviews' is a DataFrame containing the 'review' column\n",
        "# Replace 'reviews' with the name of your DataFrame and 'review' with the actual column name\n",
        "\n",
        "# Apply sentiment classification to each review\n",
        "df['Sentiment'] = df['Review'].apply(lambda x: classify_sentiment(x))\n",
        "\n",
        "# Now 'sentiment' column contains binary values (0 or 1) indicating bad or good experience\n"
      ],
      "metadata": {
        "id": "0Ql9WkmOt5dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "5jxgIK4fu-En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Sentiment']\n",
        "y"
      ],
      "metadata": {
        "id": "dkUxVcYhvg7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['ID'], axis=1)\n",
        "X"
      ],
      "metadata": {
        "id": "NbULYQHuvg3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4PdM_pKJvg1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "JpcAKplavgyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "tRyU7Obz3Tku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "#from deep_translator import GoogleTranslator\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "emoji_dictionary = {'üëç': 'thumbs_up','‚ù§Ô∏è':'red_heart','üòû':'sad_face',\"üòç\":\"love\",\"ü§¨\":\"Angry\",\"üòä\":\"Happy \",\"üíù\":\"Heart\",\"ü§ë\":\"Money\"}\n",
        "\n",
        "\n",
        "#old=pd.read_csv(\"reviews_data - reviews_data.csv\")\n",
        "WithoutStop=[]\n",
        "lowercase=[]\n",
        "panctuations=[\".\",\";\",\",\",\"?\",\"!\",\":\"]\n",
        "StopWords=[\"is\", \"was\",\"this\",\"a\",\"it\",\"it's\",\"me\",\"and\",\"to\",\"I\",\"for\",\"at\",\"of\",\"you\",\"better\"]\n",
        "WithoutPan=[]\n",
        "Translated=[]\n",
        "Noemoji=[]\n",
        "tokens=[]\n",
        "Step=[]\n",
        "lemm=[]\n",
        "for i in list(df[\"Review\"]):\n",
        "  lowercase.append(i.lower())\n",
        "  res=\"\"\n",
        "  for j in lowercase[-1]:\n",
        "    if j not in panctuations:res=res+j\n",
        "    else:res=res+\" \"\n",
        "  res = res.split()\n",
        "  tokens.append(res)\n",
        "  WithoutPan.append(res)\n",
        "  res=\"\"\n",
        "  Stemmed=\"\"\n",
        "  trans=\"\"\n",
        "  Lemmetised=\"\"\n",
        "  for k in tokens[-1]:\n",
        "    if k not in StopWords and k not in panctuations:\n",
        "      Stemmed=Stemmed+ps.stem(k)+\" \"\n",
        "      Lemmetised=Lemmetised+lemmatizer.lemmatize(k,pos=\"a\")+\" \"\n",
        "      # trans=trans+str(GoogleTranslator(source='auto', target='en').translate(k))\n",
        "      res=res+k+\" \"\n",
        "    else:\n",
        "      res=res+\" \"\n",
        "\n",
        "  new_text = \"\"\n",
        "  for k in i:\n",
        "      if k in emoji_dictionary:\n",
        "          new_text +=  emoji_dictionary[k]\n",
        "      else:\n",
        "          new_text += k\n",
        "\n",
        "  Step.append(Stemmed)\n",
        "  lemm.append(Lemmetised)\n",
        "  WithoutStop.append(res)\n",
        "  Noemoji.append(new_text)\n",
        "\n",
        "\n",
        "\n",
        "#old.drop(['Name', 'Vote1', 'Vote2',\"Title\"], axis=1, inplace=True)\n",
        "df[\"lowercase\"]=lowercase\n",
        "df[\"tokens\"]=tokens\n",
        "df[\"Stopped\"]=WithoutStop\n",
        "df[\"Pan\"]=WithoutPan\n",
        "df[\"Noemoji\"]=Noemoji\n",
        "df[\"Stemm\"]=Step\n",
        "df[\"lemmatisation\"]=lemm"
      ],
      "metadata": {
        "id": "JGAASORP2eXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "DAS78D505-5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "f-gcSr_ovgrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#checking the code\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report ,confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#lr = LogisticRegression()\n",
        "#lr.fit(X_train, y_train)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # TF-IDF vectorization\n",
        "    ('clf', LogisticRegression())  # Logistic Regression model\n",
        "])"
      ],
      "metadata": {
        "id": "PE876ZF0vgfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "# Flatten the list of lists and convert each token to lowercase\n",
        "df['tokens_lower'] = [' '.join(map(str.lower, doc)) for doc in df['tokens']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tokens_lower'], y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the TF-IDF vectorizer to the original text data and transform the training and testing data\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(X_train)  # Fit to the training data\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kCoh_5Y_-km5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "HLcDVu6OwqYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "GIGLq9fBzyw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}